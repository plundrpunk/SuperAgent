"""
E2E Test: Complete Voice-to-Validated-Feature Pipeline

Tests the full end-to-end flow from voice command to validated feature:
1. Voice command parsed by Kaya
2. Test generated by Scribe
3. Test pre-validated by Critic
4. Test executed by Runner
5. Test validated in browser by Gemini
6. Success artifacts generated

This test validates the happy path scenario where everything works correctly.
"""
import pytest
import tempfile
import shutil
import json
import time
from pathlib import Path
from unittest.mock import Mock, patch, MagicMock
from datetime import datetime

from agent_system.agents.kaya import KayaAgent
from agent_system.agents.scribe import ScribeAgent
from agent_system.agents.runner import RunnerAgent
from agent_system.agents.critic import CriticAgent
from agent_system.agents.gemini import GeminiAgent
from agent_system.router import Router
from agent_system.state.redis_client import RedisClient
from agent_system.state.vector_client import VectorClient


class TestCompletePipeline:
    """
    Complete voice-to-validated-feature pipeline tests.

    Tests the entire workflow from voice command input through validated test artifact.
    """

    @pytest.fixture(autouse=True)
    def setup_teardown(self):
        """Set up test environment."""
        # Create temporary directories
        self.temp_dir = tempfile.mkdtemp()
        self.test_output_dir = Path(self.temp_dir) / "tests"
        self.test_output_dir.mkdir(parents=True, exist_ok=True)
        self.artifacts_dir = Path(self.temp_dir) / "artifacts"
        self.artifacts_dir.mkdir(parents=True, exist_ok=True)

        # Mock state clients
        self.mock_redis = Mock(spec=RedisClient)
        self.mock_vector = Mock(spec=VectorClient)

        # Configure mocks
        self.mock_redis.health_check.return_value = True
        self.mock_redis.get.return_value = None
        self.mock_redis.set.return_value = True
        self.mock_redis.client = Mock()
        self.mock_redis.client.rpush = Mock()
        self.mock_redis.client.expire = Mock()

        self.mock_vector.search_test_patterns.return_value = []
        self.mock_vector.store_test_pattern.return_value = True

        # Session tracking
        self.session_id = f"e2e_session_{int(time.time())}"
        self.task_id = f"e2e_task_{int(time.time())}"

        yield

        # Cleanup
        shutil.rmtree(self.temp_dir, ignore_errors=True)

    def test_voice_command_to_validated_test(self):
        """
        Test complete pipeline: Voice command → Validated test

        Scenario: User says "Create test for user registration"
        Expected: Complete validated test artifact generated

        Flow:
        1. Voice command parsed by Kaya
        2. Kaya routes to Scribe
        3. Scribe generates test
        4. Critic validates test quality
        5. Runner executes test
        6. Gemini validates in browser
        7. Artifacts saved and verified
        """
        print("\n" + "="*80)
        print("TEST: Voice Command → Validated Test (Full Pipeline)")
        print("="*80)

        flow_start = time.time()
        total_cost = 0.0
        artifacts = {}

        # ===== STEP 1: Voice Command Input =====
        print("\n=== STEP 1: Voice command input ===")

        voice_command = "Create test for user registration with email and password"

        kaya = KayaAgent()
        kaya_result = kaya.execute(voice_command, context={'session_id': self.session_id})

        assert kaya_result.success, f"Kaya failed to parse command: {kaya_result.error}"
        assert kaya_result.data['action'] in ['test_created', 'route_to_scribe']

        routing = kaya_result.metadata.get('routing_decision')
        if routing:
            print(f"✓ Voice command parsed: {voice_command}")
            print(f"  Routed to: {routing.get('agent', 'scribe')}")
            print(f"  Model: {routing.get('model', 'haiku')}")
            print(f"  Complexity: {routing.get('difficulty', 'easy')}")

        # ===== STEP 2: Test Generation =====
        print("\n=== STEP 2: Test generation ===")

        scribe = ScribeAgent(vector_client=self.mock_vector)
        test_file_path = self.test_output_dir / "registration.spec.ts"

        scribe_result = scribe.execute(
            task_description="user registration with email and password",
            feature_name="User Registration",
            output_path=str(test_file_path),
            complexity='easy'
        )

        assert scribe_result.success, f"Scribe failed: {scribe_result.error}"
        assert test_file_path.exists(), "Test file should be created"

        test_content = test_file_path.read_text()
        assert 'test(' in test_content, "Test should contain test blocks"
        assert 'expect(' in test_content, "Test should contain assertions"

        scribe_cost = 0.02
        total_cost += scribe_cost
        artifacts['scribe'] = {
            'test_path': str(test_file_path),
            'cost': scribe_cost,
            'lines_of_code': len(test_content.split('\n'))
        }

        print(f"✓ Test generated: {test_file_path.name}")
        print(f"  Lines of code: {artifacts['scribe']['lines_of_code']}")
        print(f"  Cost: ${scribe_cost:.4f}")

        # ===== STEP 3: Quality Gate (Critic) =====
        print("\n=== STEP 3: Quality pre-validation ===")

        critic = CriticAgent()
        critic_result = critic.execute(str(test_file_path))

        assert critic_result.success, f"Critic failed: {critic_result.error}"
        assert critic_result.data['status'] == 'approved', \
            f"Test rejected by Critic: {critic_result.data.get('issues_found', [])}"

        critic_cost = 0.005
        total_cost += critic_cost
        artifacts['critic'] = {
            'status': critic_result.data['status'],
            'issues': critic_result.data.get('issues_found', []),
            'cost': critic_cost
        }

        print(f"✓ Quality check passed")
        print(f"  Issues found: {len(artifacts['critic']['issues'])}")
        print(f"  Cost: ${critic_cost:.4f}")

        # ===== STEP 4: Test Execution =====
        print("\n=== STEP 4: Test execution ===")

        runner = RunnerAgent()

        # Mock successful test execution
        mock_process = Mock()
        mock_process.returncode = 0
        mock_process.stdout = """
Running 1 test using 1 worker

  ✓  registration.spec.ts:15:1 › User Registration › happy path (2.8s)

  1 passed (2.8s)
"""
        mock_process.stderr = ""

        with patch('subprocess.run', return_value=mock_process):
            runner_result = runner.execute(str(test_file_path), timeout=60)

        assert runner_result.success, f"Runner failed: {runner_result.error}"
        assert runner_result.data['status'] == 'pass', "Test should pass"

        runner_cost = 0.005
        total_cost += runner_cost
        artifacts['runner'] = {
            'status': runner_result.data['status'],
            'passed_count': runner_result.data.get('passed_count', 1),
            'execution_time_s': 2.8,
            'cost': runner_cost
        }

        print(f"✓ Test executed successfully")
        print(f"  Tests passed: {artifacts['runner']['passed_count']}")
        print(f"  Execution time: {artifacts['runner']['execution_time_s']}s")
        print(f"  Cost: ${runner_cost:.4f}")

        # ===== STEP 5: Browser Validation =====
        print("\n=== STEP 5: Browser validation ===")

        gemini = GeminiAgent()

        # Mock Gemini browser validation
        mock_gemini_process = Mock()
        mock_gemini_process.returncode = 0
        mock_gemini_process.stdout = json.dumps({
            'suites': [{
                'specs': [{
                    'tests': [{
                        'results': [{
                            'status': 'passed',
                            'duration': 2800
                        }]
                    }]
                }]
            }]
        })
        mock_gemini_process.stderr = ""

        # Create mock screenshots
        screenshot_path = self.artifacts_dir / "registration_step1.png"
        screenshot_path.write_text("mock screenshot data")

        with patch('subprocess.run', return_value=mock_gemini_process):
            with patch.object(gemini, '_collect_screenshots', return_value=[str(screenshot_path)]):
                gemini_result = gemini.execute(str(test_file_path), timeout=60)

        assert gemini_result.success, f"Gemini validation failed: {gemini_result.error}"
        assert gemini_result.data['rubric_validation']['passed'], \
            f"Validation rubric failed: {gemini_result.data['rubric_validation']['errors']}"

        gemini_cost = 0.0  # No API cost for Playwright
        total_cost += gemini_cost
        artifacts['gemini'] = {
            'validation_passed': True,
            'screenshots_count': len(gemini_result.data['screenshots']),
            'rubric_score': gemini_result.data['rubric_validation'],
            'cost': gemini_cost
        }

        print(f"✓ Browser validation passed")
        print(f"  Screenshots captured: {artifacts['gemini']['screenshots_count']}")
        print(f"  Validation rubric: PASSED")

        # ===== STEP 6: Verify End-to-End Success =====
        print("\n=== STEP 6: Verify end-to-end success ===")

        flow_duration_ms = int((time.time() - flow_start) * 1000)

        # Success criteria
        success_criteria = {
            'cost_under_budget': total_cost < 0.50,
            'time_under_10min': flow_duration_ms < 600000,
            'all_agents_succeeded': all([
                kaya_result.success,
                scribe_result.success,
                critic_result.success,
                runner_result.success,
                gemini_result.success
            ]),
            'test_file_exists': test_file_path.exists(),
            'browser_validated': gemini_result.data['rubric_validation']['passed'],
            'screenshots_captured': len(gemini_result.data['screenshots']) > 0
        }

        assert all(success_criteria.values()), \
            f"Not all success criteria met: {success_criteria}"

        print(f"✓ All success criteria met:")
        for criterion, passed in success_criteria.items():
            print(f"  {criterion}: {'✓' if passed else '✗'}")

        # ===== STEP 7: Generate Pipeline Summary =====
        print("\n=== STEP 7: Pipeline summary ===")

        summary = {
            'flow': 'voice_to_validated_test',
            'voice_command': voice_command,
            'session_id': self.session_id,
            'success': True,
            'duration_ms': flow_duration_ms,
            'total_cost_usd': total_cost,
            'agents_executed': ['kaya', 'scribe', 'critic', 'runner', 'gemini'],
            'test_artifact': str(test_file_path),
            'artifacts': artifacts,
            'success_criteria': success_criteria
        }

        print(f"\n{'='*80}")
        print(f"PIPELINE COMPLETED SUCCESSFULLY")
        print(f"{'='*80}")
        print(f"Duration: {flow_duration_ms}ms ({flow_duration_ms/1000:.1f}s)")
        print(f"Total cost: ${total_cost:.4f}")
        print(f"Agents: {len(summary['agents_executed'])}")
        print(f"Test artifact: {Path(summary['test_artifact']).name}")
        print(f"{'='*80}\n")

        # Verify final assertions
        assert summary['success']
        assert total_cost < 0.50
        assert flow_duration_ms < 600000
        assert test_file_path.exists()

    def test_multiple_tests_in_session(self):
        """
        Test multiple tests created in same session.

        Scenario: User creates 3 tests in one session
        Expected: All tests created, cost tracked, budget not exceeded
        """
        print("\n" + "="*80)
        print("TEST: Multiple Tests in Session")
        print("="*80)

        kaya = KayaAgent()
        session_cost = 0.0
        test_results = []

        features = [
            "user login with OAuth",
            "profile page editing",
            "account settings"
        ]

        for i, feature in enumerate(features):
            print(f"\n--- Creating test {i+1}/3: {feature} ---")

            command = f"Create test for {feature}"
            result = kaya.execute(command, context={'session_id': self.session_id})

            assert result.success, f"Test {i+1} creation failed: {result.error}"

            session_cost += result.cost_usd
            test_results.append({
                'feature': feature,
                'success': result.success,
                'cost': result.cost_usd
            })

            print(f"✓ Test {i+1} created (cost: ${result.cost_usd:.4f})")

        # Verify session tracking
        print(f"\n=== Session Summary ===")
        print(f"Tests created: {len(test_results)}")
        print(f"Total session cost: ${session_cost:.4f}")
        print(f"Average cost per test: ${session_cost/len(test_results):.4f}")

        # Budget check
        budget_status = kaya.check_budget()
        print(f"\nBudget status: {budget_status['status']}")
        print(f"Remaining: ${budget_status['remaining']:.4f}")

        # Assertions
        assert len(test_results) == 3
        assert all(t['success'] for t in test_results)
        assert session_cost < 0.50 * len(test_results)  # Under $0.50 per test
        assert budget_status['status'] in ['ok', 'warning']

    def test_intent_parsing_accuracy(self):
        """
        Test voice intent parsing accuracy.

        Tests various command formats to ensure Kaya correctly understands intent.
        """
        print("\n" + "="*80)
        print("TEST: Voice Intent Parsing Accuracy")
        print("="*80)

        kaya = KayaAgent()

        test_cases = [
            {
                'command': "Create test for checkout flow",
                'expected_intent': 'create_test',
                'expected_feature': 'checkout flow'
            },
            {
                'command': "Write a test for user authentication",
                'expected_intent': 'create_test',
                'expected_feature': 'user authentication'
            },
            {
                'command': "Generate test for payment processing",
                'expected_intent': 'create_test',
                'expected_feature': 'payment processing'
            },
            {
                'command': "Run tests in tests/auth.spec.ts",
                'expected_intent': 'run_test',
                'expected_feature': 'tests/auth.spec.ts'
            },
            {
                'command': "Status of task abc123",
                'expected_intent': 'status',
                'expected_feature': 'abc123'
            }
        ]

        results = []
        for tc in test_cases:
            print(f"\n--- Testing: {tc['command']} ---")

            intent_result = kaya.parse_intent(tc['command'])

            success = (
                intent_result['success'] and
                intent_result['intent'] == tc['expected_intent']
            )

            results.append({
                'command': tc['command'],
                'parsed_intent': intent_result['intent'],
                'expected_intent': tc['expected_intent'],
                'success': success
            })

            status = "✓" if success else "✗"
            print(f"{status} Intent: {intent_result['intent']}")
            print(f"  Extracted: {intent_result['slots'].get('raw_value', 'N/A')}")

        # Calculate accuracy
        accuracy = sum(1 for r in results if r['success']) / len(results)

        print(f"\n=== Intent Parsing Accuracy ===")
        print(f"Accuracy: {accuracy*100:.1f}%")
        print(f"Correct: {sum(1 for r in results if r['success'])}/{len(results)}")

        # Should have high accuracy
        assert accuracy >= 0.8, f"Intent parsing accuracy too low: {accuracy*100:.1f}%"

    def test_artifact_generation_and_persistence(self):
        """
        Test that all required artifacts are generated and persisted.

        Artifacts include:
        - Test file (.spec.ts)
        - Screenshots
        - Execution logs
        - Cost tracking data
        """
        print("\n" + "="*80)
        print("TEST: Artifact Generation and Persistence")
        print("="*80)

        # Generate test through pipeline
        scribe = ScribeAgent(vector_client=self.mock_vector)
        test_path = self.test_output_dir / "artifacts_test.spec.ts"

        scribe_result = scribe.execute(
            task_description="simple form validation",
            feature_name="Form Validation",
            output_path=str(test_path),
            complexity='easy'
        )

        assert scribe_result.success
        assert test_path.exists()

        # Execute with Runner
        runner = RunnerAgent()
        mock_process = Mock()
        mock_process.returncode = 0
        mock_process.stdout = "1 passed (1.5s)"
        mock_process.stderr = ""

        with patch('subprocess.run', return_value=mock_process):
            runner_result = runner.execute(str(test_path))

        # Validate with Gemini
        gemini = GeminiAgent()
        mock_gemini_process = Mock()
        mock_gemini_process.returncode = 0
        mock_gemini_process.stdout = json.dumps({
            'suites': [{
                'specs': [{
                    'tests': [{
                        'results': [{'status': 'passed', 'duration': 1500}]
                    }]
                }]
            }]
        })

        screenshot_path = self.artifacts_dir / "validation_screenshot.png"
        screenshot_path.write_text("screenshot data")

        with patch('subprocess.run', return_value=mock_gemini_process):
            with patch.object(gemini, '_collect_screenshots', return_value=[str(screenshot_path)]):
                gemini_result = gemini.execute(str(test_path))

        # Verify artifacts
        print("\n=== Verifying Artifacts ===")

        artifacts_generated = {
            'test_file': test_path.exists(),
            'test_file_valid': test_path.exists() and len(test_path.read_text()) > 0,
            'screenshot_generated': screenshot_path.exists(),
            'execution_result': runner_result.data is not None,
            'validation_result': gemini_result.data is not None,
            'cost_tracked': (
                scribe_result.cost_usd >= 0 and
                runner_result.cost_usd >= 0 and
                gemini_result.cost_usd >= 0
            )
        }

        for artifact, generated in artifacts_generated.items():
            status = "✓" if generated else "✗"
            print(f"{status} {artifact}")

        # All artifacts should be generated
        assert all(artifacts_generated.values()), \
            f"Missing artifacts: {[k for k, v in artifacts_generated.items() if not v]}"

        print(f"\n✓ All artifacts generated and persisted successfully")


if __name__ == '__main__':
    pytest.main([__file__, '-v', '-s'])
